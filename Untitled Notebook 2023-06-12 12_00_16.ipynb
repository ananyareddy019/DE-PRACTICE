{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col, concat_ws\ndf2 = df.withColumn(\"languagesAtSchool\",\n   concat_ws(\",\",col(\"languagesAtSchool\")))\ndf2.printSchema()\ndf2.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e6467b09-aaaa-4ea1-a22b-e0d9a70463c2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\nroot\n |-- name: string (nullable = true)\n |-- languagesAtSchool: string (nullable = false)\n |-- currentState: string (nullable = true)\n\n+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.createOrReplaceTempView(\"ARRAY_STRING\")\nspark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,currentState from Array_string\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6a3e7359-2f01-4ada-8ecc-be480c02083e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StringType, ArrayType,StructType,StructField\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\n\narrayCol = ArrayType(StringType(),False)\n\ndata = [\n (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"],\"OH\",\"CA\"),\n (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"],\"NY\",\"NJ\"),\n (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"],\"UT\",\"NV\")\n]\n\nschema = StructType([ \n    StructField(\"name\",StringType(),True), \n    StructField(\"languagesAtSchool\",ArrayType(StringType()),True), \n    StructField(\"languagesAtWork\",ArrayType(StringType()),True), \n    StructField(\"currentState\", StringType(), True), \n    StructField(\"previousState\", StringType(), True) \n  ])\n\ndf = spark.createDataFrame(data=data,schema=schema)\ndf.printSchema()\ndf.show()\n\nfrom pyspark.sql.functions import explode\ndf.select(df.name,explode(df.languagesAtSchool)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"13038262-b283-473d-8c59-3cdb35ae8b4d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- languagesAtWork: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n |-- previousState: string (nullable = true)\n\n+----------------+------------------+---------------+------------+-------------+\n|            name| languagesAtSchool|languagesAtWork|currentState|previousState|\n+----------------+------------------+---------------+------------+-------------+\n|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|          OH|           CA|\n|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|          NY|           NJ|\n|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|          UT|           NV|\n+----------------+------------------+---------------+------------+-------------+\n\n+----------------+------+\n|            name|   col|\n+----------------+------+\n|    James,,Smith|  Java|\n|    James,,Smith| Scala|\n|    James,,Smith|   C++|\n|   Michael,Rose,| Spark|\n|   Michael,Rose,|  Java|\n|   Michael,Rose,|   C++|\n|Robert,,Williams|CSharp|\n|Robert,,Williams|    VB|\n+----------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.functions import split\ndf.select(split(df.name,\",\").alias(\"nameAsArray\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2e818c3b-7bc6-4512-bfaa-bc380eda5260","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|         nameAsArray|\n+--------------------+\n|    [James, , Smith]|\n|   [Michael, Rose, ]|\n|[Robert, , Williams]|\n+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.functions import array\ndf.select(df.name,array(df.currentState,df.previousState).alias(\"States\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"718bbb71-8214-4b28-9537-9f4bca677ec3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------+--------+\n|            name|  States|\n+----------------+--------+\n|    James,,Smith|[OH, CA]|\n|   Michael,Rose,|[NY, NJ]|\n|Robert,,Williams|[UT, NV]|\n+----------------+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import array_contains\ndf.select(df.name,array_contains(df.languagesAtSchool,\"Java\")\n    .alias(\"array_contains\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8de9ad0f-93d0-43ba-9054-29b482286d0d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------+--------------+\n|            name|array_contains|\n+----------------+--------------+\n|    James,,Smith|          true|\n|   Michael,Rose,|          true|\n|Robert,,Williams|         false|\n+----------------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\ndataCollect = deptDF.collect()\n\nprint(dataCollect)\n\ndataCollect2 = deptDF.select(\"dept_name\").collect()\nprint(dataCollect2)\n\nfor row in dataCollect:\n    print(row['dept_name'] + \",\" +str(row['dept_id']))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"35ef5fc4-f1f6-46a7-8e9e-7e9b8ed66aa7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n[Row(dept_name='Finance', dept_id=10), Row(dept_name='Marketing', dept_id=20), Row(dept_name='Sales', dept_id=30), Row(dept_name='IT', dept_id=40)]\n[Row(dept_name='Finance'), Row(dept_name='Marketing'), Row(dept_name='Sales'), Row(dept_name='IT')]\nFinance,10\nMarketing,20\nSales,30\nIT,40\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6422d065-f15f-4486-a49a-24309c1ce837","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-12 12:00:16","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
