{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\ndata = [\n    (\"James\",None,\"M\"),\n    (\"Anna\",\"NY\",\"F\"),\n    (\"Julia\",None,None)\n  ]\n\ncolumns = [\"name\",\"state\",\"gender\"]\ndf = spark.createDataFrame(data,columns)\ndf.show()\n\ndf.filter(\"state is NULL\").show()\ndf.filter(df.state.isNull()).show()\ndf.filter(col(\"state\").isNull()).show() \n\ndf.na.drop(\"state\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7a35561d-ab26-41d0-8be4-debaea6a6f15","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n| Anna|   NY|     F|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n+-----+-----+------+\n| name|state|gender|\n+-----+-----+------+\n|James| null|     M|\n|Julia| null|  null|\n+-----+-----+------+\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-303549684801348>:22\u001B[0m\n\u001B[1;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     20\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow() \n\u001B[0;32m---> 22\u001B[0m df\u001B[38;5;241m.\u001B[39mna\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5268\u001B[0m, in \u001B[0;36mDataFrameNaFunctions.drop\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n\u001B[1;32m   5262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[1;32m   5263\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5264\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5265\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5266\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n\u001B[0;32m-> 5268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthresh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3968\u001B[0m, in \u001B[0;36mDataFrame.dropna\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n\u001B[1;32m   3925\u001B[0m \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\u001B[39;00m\n\u001B[1;32m   3926\u001B[0m \u001B[38;5;124;03m:func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\u001B[39;00m\n\u001B[1;32m   3927\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3965\u001B[0m \u001B[38;5;124;03m+---+------+-----+\u001B[39;00m\n\u001B[1;32m   3966\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   3967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m how \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m-> 3968\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m how \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) should be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   3970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3971\u001B[0m     subset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\n\u001B[0;31mValueError\u001B[0m: how ('state') should be 'any' or 'all'","errorSummary":"<span class='ansi-red-fg'>ValueError</span>: how ('state') should be 'any' or 'all'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n","File \u001B[0;32m<command-303549684801348>:22\u001B[0m\n","\u001B[1;32m     19\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(df\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow()\n","\u001B[1;32m     20\u001B[0m df\u001B[38;5;241m.\u001B[39mfilter(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39misNull())\u001B[38;5;241m.\u001B[39mshow() \n","\u001B[0;32m---> 22\u001B[0m df\u001B[38;5;241m.\u001B[39mna\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:5268\u001B[0m, in \u001B[0;36mDataFrameNaFunctions.drop\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n","\u001B[1;32m   5262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n","\u001B[1;32m   5263\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n","\u001B[1;32m   5264\u001B[0m     how: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n","\u001B[1;32m   5265\u001B[0m     thresh: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   5266\u001B[0m     subset: Optional[Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m], List[\u001B[38;5;28mstr\u001B[39m]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n","\u001B[1;32m   5267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame:\n","\u001B[0;32m-> 5268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthresh\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthresh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubset\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:43\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     39\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n","\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n","\u001B[1;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(_local, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogging\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m _local\u001B[38;5;241m.\u001B[39mlogging:\n","\u001B[1;32m     42\u001B[0m         \u001B[38;5;66;03m# no need to log since this should be internal call.\u001B[39;00m\n","\u001B[0;32m---> 43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     44\u001B[0m     _local\u001B[38;5;241m.\u001B[39mlogging \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n","\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3968\u001B[0m, in \u001B[0;36mDataFrame.dropna\u001B[0;34m(self, how, thresh, subset)\u001B[0m\n","\u001B[1;32m   3925\u001B[0m \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` omitting rows with null values.\u001B[39;00m\n","\u001B[1;32m   3926\u001B[0m \u001B[38;5;124;03m:func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\u001B[39;00m\n","\u001B[1;32m   3927\u001B[0m \n","\u001B[0;32m   (...)\u001B[0m\n","\u001B[1;32m   3965\u001B[0m \u001B[38;5;124;03m+---+------+-----+\u001B[39;00m\n","\u001B[1;32m   3966\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n","\u001B[1;32m   3967\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m how \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m how \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n","\u001B[0;32m-> 3968\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m how \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m) should be \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124many\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\u001B[1;32m   3970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m subset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n","\u001B[1;32m   3971\u001B[0m     subset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n","\n","\u001B[0;31mValueError\u001B[0m: how ('state') should be 'any' or 'all'"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"752707c1-b10f-41e3-b3cc-895fbf703e3d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-fulter-null.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
