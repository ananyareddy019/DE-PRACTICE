{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [\"Project\",\n\"Gutenberg’s\",\n\"Alice’s\",\n\"Adventures\",\n\"in\",\n\"Wonderland\",\n\"Project\",\n\"Gutenberg’s\",\n\"Adventures\",\n\"in\",\n\"Wonderland\",\n\"Project\",\n\"Gutenberg’s\"]\n\nrdd=spark.sparkContext.parallelize(data)\n\nrdd2=rdd.map(lambda x: (x,1))\nfor element in rdd2.collect():\n    print(element)\n    \ndata = [('James','Smith','M',30),\n  ('Anna','Rose','F',41),\n  ('Robert','Williams','M',62), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\nrdd2=df.rdd.map(lambda x: \n    (x[0]+\",\"+x[1],x[2],x[3]*2)\n    )  \ndf2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\ndf2.show()\n\n\n#Referring Column Names\nrdd2=df.rdd.map(lambda x: \n    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n    ) \n\n\n#Referring Column Names\nrdd2=df.rdd.map(lambda x: \n    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n    ) \n\n\ndef func1(x):\n    firstName=x.firstname\n    lastName=x.lastname\n    name=firstName+\",\"+lastName\n    gender=x.gender.lower()\n    salary=x.salary*2\n    return (name,gender,salary)\n\nrdd2=df.rdd.map(lambda x: func1(x)).toDF().show()\nrdd2=df.rdd.map(func1).toDF().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11f306ad-ab42-451a-aca3-1b75f8d3fbb4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-rdd-map.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
