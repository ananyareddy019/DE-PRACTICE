{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nfrom pyspark.sql.functions import col,expr\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\ndf=spark.createDataFrame(data,(\"date\",\"increment\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fe2a8232-4762-4e1b-a697-3e0bc72ee900","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ea5df6ae-5f64-4bf0-89e3-2c229c18ef16","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+---------+\n|      date|increment|\n+----------+---------+\n|2019-01-23|        1|\n|2019-06-24|        2|\n|2019-09-20|        3|\n+----------+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nfrom pyspark.sql.functions import col,expr\ndata=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\nspark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n    .select(col(\"date\"),col(\"increment\"), \\\n      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01681d2e-3150-4b0a-a59a-c8e58b9ffef5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"673369c8-9259-43f8-b647-16478a618ecc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import concat_ws\ndf.withColumn(\"name\", concat_ws(\",\",\"firstname\",'lastname')) \\\n  .show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"00974959-873e-49cd-970d-bbe74470b375","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+---------------+\n|firstname|lastname|gender|salary|           name|\n+---------+--------+------+------+---------------+\n|    James|   Smith|     M|  3000|    James,Smith|\n|     Anna|    Rose|     F|  4100|      Anna,Rose|\n|   Robert|Williams|     M|  6200|Robert,Williams|\n+---------+--------+------+------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.createOrReplaceTempView(\"PER\")\nspark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4a0dd83f-ca99-4e21-9038-55c90ac759d4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+------+-----+\n|firstname|salary|bonus|\n+---------+------+-----+\n|    James|  3000|  0.3|\n|     Anna|  4100|  0.3|\n|   Robert|  6200|  0.3|\n+---------+------+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\", \"Sales\", 3000),\n    (\"Michael\", \"Sales\", 4600),\n    (\"Robert\", \"Sales\", 4100),\n    (\"Maria\", \"Finance\", 3000),\n    (\"James\", \"Sales\", 3000),\n    (\"Scott\", \"Finance\", 3300),\n    (\"Jen\", \"Finance\", 3900),\n    (\"Jeff\", \"Marketing\", 3000),\n    (\"Kumar\", \"Marketing\", 2000),\n    (\"Saif\", \"Sales\", 4100)\n  ]\nschema = [\"employee_name\", \"department\", \"salary\"]\n  \n  \ndf = spark.createDataFrame(data=simpleData, schema = schema)\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b4acf9f2-cf58-4882-b730-e17a7dd3f1fc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"81186ddd-320f-43bd-81b1-5c9f950af390","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["print(\"approx_count_distinct: \" + \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4e429262-d3db-477d-ab24-9276b13c2a55","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-852034209064240>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapprox_count_distinct: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[0;32m----> 2\u001B[0m       \u001B[38;5;28mstr\u001B[39m(df\u001B[38;5;241m.\u001B[39mselect(\u001B[43mapprox_count_distinct\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]))\n\n\u001B[0;31mNameError\u001B[0m: name 'approx_count_distinct' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'approx_count_distinct' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-852034209064240>:2\u001B[0m\n","\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapprox_count_distinct: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n","\u001B[0;32m----> 2\u001B[0m       \u001B[38;5;28mstr\u001B[39m(df\u001B[38;5;241m.\u001B[39mselect(\u001B[43mapprox_count_distinct\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]))\n","\n","\u001B[0;31mNameError\u001B[0m: name 'approx_count_distinct' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["print(\"approx_count_distinct: \" + \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f7431ca1-0b58-4a1a-9c71-5f0818d07314","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-852034209064241>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapprox_count_distinct: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n\u001B[0;32m----> 2\u001B[0m       \u001B[38;5;28mstr\u001B[39m(df\u001B[38;5;241m.\u001B[39mselect(\u001B[43mapprox_count_distinct\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]))\n\n\u001B[0;31mNameError\u001B[0m: name 'approx_count_distinct' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'approx_count_distinct' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-852034209064241>:2\u001B[0m\n","\u001B[1;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapprox_count_distinct: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \\\n","\u001B[0;32m----> 2\u001B[0m       \u001B[38;5;28mstr\u001B[39m(df\u001B[38;5;241m.\u001B[39mselect(\u001B[43mapprox_count_distinct\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39mcollect()[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]))\n","\n","\u001B[0;31mNameError\u001B[0m: name 'approx_count_distinct' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import approx_count_distinct,collect_list\nfrom pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\nfrom pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \nfrom pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\nfrom pyspark.sql.functions import variance,var_samp,  var_pop\nprint(\"approx_count_distinct: \" + \\\n      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f4c10746-95e9-4884-94ab-20bc0fa7945e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["approx_count_distinct: 6\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select(approx_count_distinct(\"salary\")).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"228898af-c9d4-44d9-a957-49d43b9e67a1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[14]: [Row(approx_count_distinct(salary)=6)]"]}],"execution_count":0},{"cell_type":"code","source":["df.select(approx_count_distinct(\"salary\").collect()[0])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"055c6050-203a-4892-a4fd-d4133408b68e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-852034209064244>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df\u001B[38;5;241m.\u001B[39mselect(\u001B[43mapprox_count_distinct\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msalary\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m])\n\n\u001B[0;31mTypeError\u001B[0m: 'Column' object is not callable","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: 'Column' object is not callable","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-852034209064244>:1\u001B[0m\n","\u001B[0;32m----> 1\u001B[0m df\u001B[38;5;241m.\u001B[39mselect(\u001B[43mapprox_count_distinct\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msalary\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m])\n","\n","\u001B[0;31mTypeError\u001B[0m: 'Column' object is not callable"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ncolumns = [\"name\",\"languagesAtSchool\",\"currentState\"]\ndata = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\ndf.printSchema()\ndf.show(truncate=False)\n\nfrom pyspark.sql.functions import col, concat_ws\ndf2 = df.withColumn(\"languagesAtSchool\",\n   concat_ws(\",\",col(\"languagesAtSchool\")))\ndf2.printSchema()\ndf2.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"58d4b330-2fd4-42ce-b7b9-64c6e118e0dc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- languagesAtSchool: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- currentState: string (nullable = true)\n\n+----------------+------------------+------------+\n|name            |languagesAtSchool |currentState|\n+----------------+------------------+------------+\n|James,,Smith    |[Java, Scala, C++]|CA          |\n|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n|Robert,,Williams|[CSharp, VB]      |NV          |\n+----------------+------------------+------------+\n\nroot\n |-- name: string (nullable = true)\n |-- languagesAtSchool: string (nullable = false)\n |-- currentState: string (nullable = true)\n\n+----------------+-----------------+------------+\n|name            |languagesAtSchool|currentState|\n+----------------+-----------------+------------+\n|James,,Smith    |Java,Scala,C++   |CA          |\n|Michael,Rose,   |Spark,Java,C++   |NJ          |\n|Robert,,Williams|CSharp,VB        |NV          |\n+----------------+-----------------+------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"41462ddb-a1cd-4817-b4c6-f5e5c47384c0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-12 11:31:20","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
