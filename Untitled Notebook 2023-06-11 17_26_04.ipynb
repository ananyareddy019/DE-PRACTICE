{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n  ]\ncolumns=[\"firstname\",\"lastname\",\"country\",\"state\"]\ndf=spark.createDataFrame(data=data,schema=columns)\ndf.show()\nprint(df.collect())\n\nstates1=df.rdd.map(lambda x: x[3]).collect()\nprint(states1)\n#['CA', 'NY', 'CA', 'FL']\nfrom collections import OrderedDict \nres = list(OrderedDict.fromkeys(states1)) \nprint(res)\n#['CA', 'NY', 'FL']\n\n\n#Example 2\nstates2=df.rdd.map(lambda x: x.state).collect()\nprint(states2)\n#['CA', 'NY', 'CA', 'FL']\n\nstates3=df.select(df.state).collect()\nprint(states3)\n#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n\nstates4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\nprint(states4)\n#['CA', 'NY', 'CA', 'FL']\n\nstates5=df.select(df.state).toPandas()['state']\nstates6=list(states5)\nprint(states6)\n#['CA', 'NY', 'CA', 'FL']\n\npandDF=df.select(df.state,df.firstname).toPandas()\nprint(list(pandDF['state']))\nprint(list(pandDF['firstname']))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bf04307b-9728-4b44-9246-8077b048dd0d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n['CA', 'NY', 'CA', 'FL']\n['CA', 'NY', 'FL']\n['CA', 'NY', 'CA', 'FL']\n[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n['CA', 'NY', 'CA', 'FL']\n['CA', 'NY', 'CA', 'FL']\n['CA', 'NY', 'CA', 'FL']\n['James', 'Michael', 'Robert', 'Maria']\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[1]\") \\\n                    .appName('SparkByExamples.com') \\\n                    .getOrCreate()\n\ndata = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n  ]\ncolumns=[\"firstname\",\"lastname\",\"country\",\"state\"]\ndf=spark.createDataFrame(data=data,schema=columns)\ndf.show()\nprint(df.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"784918f1-cb69-4dee-909a-1dbd3f4c0f7b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n"]}],"execution_count":0},{"cell_type":"code","source":["states1=df.rdd.map(lambda x: x[3]).collect()\nprint(states1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ffaa5b1f-4a1e-4ebf-98d6-d019070e183f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["['CA', 'NY', 'CA', 'FL']\n"]}],"execution_count":0},{"cell_type":"code","source":["type(states1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c4a0bebb-60a4-4edb-976d-ac6476012a11","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[7]: list"]}],"execution_count":0},{"cell_type":"code","source":["from collections import OrderedDict \nres = list(OrderedDict.fromkeys(states1)) \nprint(res)\n#['CA', 'NY', 'FL']\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6a26626b-eafe-45d9-8a3f-1d4362e64e8c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["['CA', 'NY', 'FL']\n"]}],"execution_count":0},{"cell_type":"code","source":["states3=df.select(df.state).collect()\nprint(states3)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bb8a30a6-eea3-4dc4-b71b-6e746c02c105","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n"]}],"execution_count":0},{"cell_type":"code","source":["states2=df.rdd.map(lambda x: x.state).collect()\nprint(states2)\n#['CA', 'NY', 'CA', 'FL']"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"aedbcff8-3ce6-4c21-b612-931b6a95f022","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["['CA', 'NY', 'CA', 'FL']\n"]}],"execution_count":0},{"cell_type":"code","source":["print(df.state)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7149db3e-03fb-4eda-b864-088884475856","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Column<'state'>\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select(df.state).toPandas()[]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b1daf13f-c348-45f6-ad29-b7d2180a0354","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CA</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>FL</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>state</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CA</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NY</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CA</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>FL</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]}}],"execution_count":0},{"cell_type":"code","source":["states2=df.rdd.map(lambda x: x.state).collect()\nprint(states2)\n#['CA', 'NY', 'CA', 'FL']\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a26acc54-fad8-432d-901f-2d88765c2173","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["['CA', 'NY', 'CA', 'FL']\n"]}],"execution_count":0},{"cell_type":"code","source":["df.rdd.flatMap(lambda x: x).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cf1137fc-f98b-4d90-80a3-040b42f34ef2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[21]: ['James',\n 'Smith',\n 'USA',\n 'CA',\n 'Michael',\n 'Rose',\n 'USA',\n 'NY',\n 'Robert',\n 'Williams',\n 'USA',\n 'CA',\n 'Maria',\n 'Jones',\n 'USA',\n 'FL']"]}],"execution_count":0},{"cell_type":"code","source":["df.rdd.flatMap(lambda x: x.state).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b56ec8aa-3b9e-4de5-bf1e-193ce8180817","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[23]: ['C', 'A', 'N', 'Y', 'C', 'A', 'F', 'L']"]}],"execution_count":0},{"cell_type":"code","source":["\ndf.rdd.flatMap(lambda x: x.state.split(\" \")).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c629b42b-ecb2-4fcc-a068-0750f12f0e4b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[20]: ['CA', 'NY', 'CA', 'FL']"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import to_timestamp, current_timestamp\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nschema = StructType([\n            StructField(\"seq\", StringType(), True)])\n\ndates = ['1']\n\ndf = spark.createDataFrame(list('1',), schema=schema)\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c3ca9735-c8b2-43c6-a908-e93f6e73f4b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-4319768239186990>:14\u001B[0m\n\u001B[1;32m      9\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n\u001B[1;32m     10\u001B[0m             StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseq\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)])\n\u001B[1;32m     12\u001B[0m dates \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 14\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m,), schema\u001B[38;5;241m=\u001B[39mschema)\n\u001B[1;32m     16\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n\u001B[1;32m   1215\u001B[0m     )\n\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1264\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1266\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1267\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n\u001B[1;32m   1268\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m    882\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n\u001B[1;32m    883\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 888\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:858\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n\u001B[1;32m    854\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n\u001B[1;32m    855\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m--> 858\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m    861\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1232\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n\u001B[0;32m-> 1232\u001B[0m     \u001B[43mverify_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1867\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1865\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1866\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n\u001B[0;32m-> 1867\u001B[0m         \u001B[43mverify_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1851\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n\u001B[1;32m   1849\u001B[0m         verifier(d\u001B[38;5;241m.\u001B[39mget(f))\n\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1851\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   1852\u001B[0m         new_msg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStructType can not accept object \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m in type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (obj, \u001B[38;5;28mtype\u001B[39m(obj)))\n\u001B[1;32m   1853\u001B[0m     )\n\n\u001B[0;31mTypeError\u001B[0m: StructType can not accept object '1' in type <class 'str'>","errorSummary":"<span class='ansi-red-fg'>TypeError</span>: StructType can not accept object '1' in type <class 'str'>","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-4319768239186990>:14\u001B[0m\n","\u001B[1;32m      9\u001B[0m schema \u001B[38;5;241m=\u001B[39m StructType([\n","\u001B[1;32m     10\u001B[0m             StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseq\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)])\n","\u001B[1;32m     12\u001B[0m dates \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m]\n","\u001B[0;32m---> 14\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m,), schema\u001B[38;5;241m=\u001B[39mschema)\n","\u001B[1;32m     16\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1216\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n","\u001B[1;32m   1211\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_pandas \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n","\u001B[1;32m   1212\u001B[0m     \u001B[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001B[39;00m\n","\u001B[1;32m   1213\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(SparkSession, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mcreateDataFrame(  \u001B[38;5;66;03m# type: ignore[call-overload]\u001B[39;00m\n","\u001B[1;32m   1214\u001B[0m         data, schema, samplingRatio, verifySchema\n","\u001B[1;32m   1215\u001B[0m     )\n","\u001B[0;32m-> 1216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_dataframe\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplingRatio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverifySchema\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n","\u001B[1;32m   1218\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1266\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n","\u001B[1;32m   1264\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_createFromRDD(data\u001B[38;5;241m.\u001B[39mmap(prepare), schema, samplingRatio)\n","\u001B[1;32m   1265\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;32m-> 1266\u001B[0m     rdd, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_createFromLocal\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprepare\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1267\u001B[0m jrdd \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mSerDeUtil\u001B[38;5;241m.\u001B[39mtoJavaArray(rdd\u001B[38;5;241m.\u001B[39m_to_java_object_rdd())\n","\u001B[1;32m   1268\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsparkSession\u001B[38;5;241m.\u001B[39mapplySchemaToPythonRDD(jrdd\u001B[38;5;241m.\u001B[39mrdd(), struct\u001B[38;5;241m.\u001B[39mjson())\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:888\u001B[0m, in \u001B[0;36mSparkSession._createFromLocal\u001B[0;34m(self, data, schema)\u001B[0m\n","\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_createFromLocal\u001B[39m(\n","\u001B[1;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n","\u001B[1;32m    882\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRDD[Tuple]\u001B[39m\u001B[38;5;124m\"\u001B[39m, StructType]:\n","\u001B[1;32m    883\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n","\u001B[1;32m    884\u001B[0m \u001B[38;5;124;03m    Create an RDD for DataFrame from a list or pandas.DataFrame, returns the RDD and schema.\u001B[39;00m\n","\u001B[1;32m    885\u001B[0m \u001B[38;5;124;03m    This would be broken with table acl enabled as user process does not have permission to\u001B[39;00m\n","\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    write temp files.\u001B[39;00m\n","\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n","\u001B[0;32m--> 888\u001B[0m     internal_data, struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wrap_data_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sc\u001B[38;5;241m.\u001B[39mparallelize(internal_data), struct\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:858\u001B[0m, in \u001B[0;36mSparkSession._wrap_data_schema\u001B[0;34m(self, data, schema)\u001B[0m\n","\u001B[1;32m    853\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_wrap_data_schema\u001B[39m(\n","\u001B[1;32m    854\u001B[0m     \u001B[38;5;28mself\u001B[39m, data: Iterable[Any], schema: Optional[Union[DataType, List[\u001B[38;5;28mstr\u001B[39m]]]\n","\u001B[1;32m    855\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Iterable[Tuple], StructType]:\n","\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# make sure data could consumed multiple times\u001B[39;00m\n","\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mlist\u001B[39m):\n","\u001B[0;32m--> 858\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m schema \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n","\u001B[1;32m    861\u001B[0m         struct \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferSchemaFromList(data, names\u001B[38;5;241m=\u001B[39mschema)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1232\u001B[0m, in \u001B[0;36mSparkSession._create_dataframe.<locals>.prepare\u001B[0;34m(obj)\u001B[0m\n","\u001B[1;32m   1230\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n","\u001B[1;32m   1231\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(obj):\n","\u001B[0;32m-> 1232\u001B[0m     \u001B[43mverify_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1233\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1867\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify\u001B[0;34m(obj)\u001B[0m\n","\u001B[1;32m   1865\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mverify\u001B[39m(obj: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n","\u001B[1;32m   1866\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_nullability(obj):\n","\u001B[0;32m-> 1867\u001B[0m         \u001B[43mverify_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1851\u001B[0m, in \u001B[0;36m_make_type_verifier.<locals>.verify_struct\u001B[0;34m(obj)\u001B[0m\n","\u001B[1;32m   1849\u001B[0m         verifier(d\u001B[38;5;241m.\u001B[39mget(f))\n","\u001B[1;32m   1850\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[0;32m-> 1851\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n","\u001B[1;32m   1852\u001B[0m         new_msg(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStructType can not accept object \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m in type \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (obj, \u001B[38;5;28mtype\u001B[39m(obj)))\n","\u001B[1;32m   1853\u001B[0m     )\n","\n","\u001B[0;31mTypeError\u001B[0m: StructType can not accept object '1' in type <class 'str'>"]}}],"execution_count":0},{"cell_type":"code","source":["dates = [['1']]\nlist(dates)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e4da1acd-de94-4833-b6ad-45b6de13bcbe","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[4]: [['1']]"]}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame(dates, schema=schema)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"054192c3-d6a5-42e9-92ef-2ca81b2a1083","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"980139dc-c39d-4185-a2ab-20477e4c9b88","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+\n|seq|\n+---+\n|  1|\n+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd    \ndata = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n  \n# Create the pandas DataFrame \npandasDF = pd.DataFrame(data, columns = ['Name', 'Age']) \n  \n# print dataframe. \nprint(pandasDF)\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[1]\") \\\n    .appName(\"SparkByExamples.com\") \\\n    .getOrCreate()\n\nsparkDF=spark.createDataFrame(pandasDF) \nsparkDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"df062ce8-efe7-4465-bfb9-fd8f4a0e516d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"stream","output_type":"stream","name":"stdout","text":["     Name  Age\n0   Scott   50\n1    Jeff   45\n2  Thomas   54\n3     Ann   34\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField, StringType, IntegerType\nmySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n                       ,StructField(\"Age\", IntegerType(), True)])\n\nsparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\nsparkDF2.printSchema()\nsparkDF2.show()\n\n\nspark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n\npandasDF2=sparkDF2.select(\"*\").toPandas\nprint(pandasDF2)\n\n\ntest=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\nprint(test)\n\ntest123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\nprint(test123)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"95444839-d074-42d4-a58f-69a2e2e14833","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- First Name: string (nullable = true)\n |-- Age: integer (nullable = true)\n\n+----------+---+\n|First Name|Age|\n+----------+---+\n|     Scott| 50|\n|      Jeff| 45|\n|    Thomas| 54|\n|       Ann| 34|\n+----------+---+\n\n<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\ntrue\ntrue\n"]}],"execution_count":0},{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import approx_count_distinct,collect_list\nfrom pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\nfrom pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \nfrom pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\nfrom pyspark.sql.functions import variance,var_samp,  var_pop\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"541cda62-0602-42f0-8c87-9271d517d583","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Untitled Notebook 2023-06-11 17:26:04","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
