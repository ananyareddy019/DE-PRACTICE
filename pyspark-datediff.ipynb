{"cells":[{"cell_type":"code","source":["spark = SparkSession.builder \\\n          .appName('SparkByExamples.com') \\\n          .getOrCreate()\ndata = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\n\ndf=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\n\nfrom pyspark.sql.functions import *\n\ndf.select(\n      col(\"date\"),\n      current_date().alias(\"current_date\"),\n      datediff(current_date(),col(\"date\")).alias(\"datediff\")\n    ).show()\n\ndf.withColumn(\"datesDiff\", datediff(current_date(),col(\"date\"))) \\\n  .withColumn(\"montsDiff\", months_between(current_date(),col(\"date\"))) \\\n  .withColumn(\"montsDiff_round\",round(months_between(current_date(),col(\"date\")),2)) \\\n  .withColumn(\"yearsDiff\",months_between(current_date(),col(\"date\"))/lit(12)) \\\n  .withColumn(\"yearsDiff_round\",round(months_between(current_date(),col(\"date\"))/lit(12),2)) \\\n  .show()\n\ndata2 = [(\"1\",\"07-01-2019\"),(\"2\",\"06-24-2019\"),(\"3\",\"08-24-2019\")]  \ndf2=spark.createDataFrame(data=data2,schema=[\"id\",\"date\"])\ndf2.select(\n    to_date(col(\"date\"),\"MM-dd-yyyy\").alias(\"date\"),\n    current_date().alias(\"endDate\")\n    )\n\n#SQL\n\nspark.sql(\"select round(months_between('2019-07-01',current_date())/12,2) as years_diff\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"32e9adef-6add-40f2-b5b3-10bc97306ead","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-303549684801315>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n\u001B[1;32m      2\u001B[0m           \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSparkByExamples.com\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m      3\u001B[0m           \u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m      4\u001B[0m data \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2019-07-01\u001B[39m\u001B[38;5;124m\"\u001B[39m),(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2019-06-24\u001B[39m\u001B[38;5;124m\"\u001B[39m),(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2019-08-24\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[1;32m      6\u001B[0m df\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m=\u001B[39mdata,schema\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\n\u001B[0;31mNameError\u001B[0m: name 'SparkSession' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'SparkSession' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-303549684801315>:1\u001B[0m\n","\u001B[0;32m----> 1\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n","\u001B[1;32m      2\u001B[0m           \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSparkByExamples.com\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n","\u001B[1;32m      3\u001B[0m           \u001B[38;5;241m.\u001B[39mgetOrCreate()\n","\u001B[1;32m      4\u001B[0m data \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2019-07-01\u001B[39m\u001B[38;5;124m\"\u001B[39m),(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2019-06-24\u001B[39m\u001B[38;5;124m\"\u001B[39m),(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2019-08-24\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n","\u001B[1;32m      6\u001B[0m df\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mcreateDataFrame(data\u001B[38;5;241m=\u001B[39mdata,schema\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdate\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n","\n","\u001B[0;31mNameError\u001B[0m: name 'SparkSession' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d1264b2-cc20-4efb-8f2c-457453a939f8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-datediff","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
