{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField, StringType\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nschema = StructType([\n  StructField('firstname', StringType(), True),\n  StructField('middlename', StringType(), True),\n  StructField('lastname', StringType(), True)\n  ])\ndf = spark.createDataFrame(spark.sparkContext.emptyRDD(),schema)\ndf.printSchema()\n\ndf1 = spark.sparkContext.parallelize([]).toDF(schema)\ndf1.printSchema()\n\ndf2 = spark.createDataFrame([], schema)\ndf2.printSchema()\n\ndf3 = spark.emptyDataFrame()\ndf3.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a7537b99-7f19-483c-8d7a-3a86d42e4f47","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\nroot\n |-- firstname: string (nullable = true)\n |-- middlename: string (nullable = true)\n |-- lastname: string (nullable = true)\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-303549684801327>:21\u001B[0m\n\u001B[1;32m     18\u001B[0m df2 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([], schema)\n\u001B[1;32m     19\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[0;32m---> 21\u001B[0m df3 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39memptyDataFrame()\n\u001B[1;32m     22\u001B[0m df3\u001B[38;5;241m.\u001B[39mprintSchema()\n\n\u001B[0;31mAttributeError\u001B[0m: 'SparkSession' object has no attribute 'emptyDataFrame'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'SparkSession' object has no attribute 'emptyDataFrame'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n","File \u001B[0;32m<command-303549684801327>:21\u001B[0m\n","\u001B[1;32m     18\u001B[0m df2 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mcreateDataFrame([], schema)\n","\u001B[1;32m     19\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n","\u001B[0;32m---> 21\u001B[0m df3 \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39memptyDataFrame()\n","\u001B[1;32m     22\u001B[0m df3\u001B[38;5;241m.\u001B[39mprintSchema()\n","\n","\u001B[0;31mAttributeError\u001B[0m: 'SparkSession' object has no attribute 'emptyDataFrame'"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81e3a628-26f7-44a8-bdd7-baa4edf9929e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-empty-dataset","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
