{"cells":[{"cell_type":"code","source":["import pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, asc,desc\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n  ]\ncolumns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n\ndf.printSchema()\ndf.show(truncate=False)\n\ndf.sort(\"department\",\"state\").show(truncate=False)\ndf.sort(col(\"department\"),col(\"state\")).show(truncate=False)\n\ndf.orderBy(\"department\",\"state\").show(truncate=False)\ndf.orderBy(col(\"department\"),col(\"state\")).show(truncate=False)\n\ndf.sort(df.department.asc(),df.state.asc()).show(truncate=False)\ndf.sort(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\ndf.orderBy(col(\"department\").asc(),col(\"state\").asc()).show(truncate=False)\n\ndf.sort(df.department.asc(),df.state.desc()).show(truncate=False)\ndf.sort(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\ndf.orderBy(col(\"department\").asc(),col(\"state\").desc()).show(truncate=False)\n\n\ndf.createOrReplaceTempView(\"EMP\")\ndf.select(\"employee_name\",asc(\"department\"),desc(\"state\"),\"salary\",\"age\",\"bonus\").show(truncate=False)\n\nspark.sql(\"select employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fabe75e0-a237-48ab-b61e-ed3f49d01146","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|James        |Sales     |NY   |90000 |34 |10000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n+-------------+----------+-----+------+---+-----+\n\n+-------------+----------+-----+------+---+-----+\n|employee_name|department|state|salary|age|bonus|\n+-------------+----------+-----+------+---+-----+\n|Jen          |Finance   |NY   |79000 |53 |15000|\n|Scott        |Finance   |NY   |83000 |36 |19000|\n|Maria        |Finance   |CA   |90000 |24 |23000|\n|Raman        |Finance   |CA   |99000 |40 |24000|\n|Kumar        |Marketing |NY   |91000 |50 |21000|\n|Jeff         |Marketing |CA   |80000 |25 |18000|\n|James        |Sales     |NY   |90000 |34 |10000|\n|Michael      |Sales     |NY   |86000 |56 |20000|\n|Robert       |Sales     |CA   |81000 |30 |23000|\n+-------------+----------+-----+------+---+-----+\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-303549684801381>:40\u001B[0m\n\u001B[1;32m     36\u001B[0m df\u001B[38;5;241m.\u001B[39morderBy(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39masc(),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdesc())\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     39\u001B[0m df\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEMP\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 40\u001B[0m df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memployee_name\u001B[39m\u001B[38;5;124m\"\u001B[39m,asc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m),desc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m),\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbonus\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     42\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:933\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    924\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m    925\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m    926\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    927\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    930\u001B[0m         },\n\u001B[1;32m    931\u001B[0m     )\n\u001B[0;32m--> 933\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mint_truncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o485.showString.\n: org.apache.spark.SparkException: [INTERNAL_ERROR] Cannot generate code for expression: input[1, string, true] ASC NULLS FIRST\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:78)\n\tat org.apache.spark.SparkException$.internalError(SparkException.scala:82)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGenerateCodeForExpressionError(QueryExecutionErrors.scala:74)\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:489)\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:488)\n\tat org.apache.spark.sql.catalyst.expressions.SortOrder.doGenCode(SortOrder.scala:63)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:235)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$2(Expression.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:211)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCodeInternal(namedExpressions.scala:163)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:269)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:269)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:267)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpression(CodeGenerator.scala:1450)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$2(CodeGenerator.scala:1531)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$1(CodeGenerator.scala:1529)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1183)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressionsForWholeStageWithCSE(CodeGenerator.scala:1529)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:197)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:152)\n\tat org.apache.spark.sql.execution.RDDScanExec.consume(ExistingRDD.scala:297)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:488)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:461)\n\tat org.apache.spark.sql.execution.RDDScanExec.doProduce(ExistingRDD.scala:297)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:98)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:108)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:334)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:330)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:93)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:93)\n\tat org.apache.spark.sql.execution.RDDScanExec.produce(ExistingRDD.scala:297)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:56)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:98)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:108)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:334)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:330)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:93)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:93)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:46)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:661)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:724)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:277)\n\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:108)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:334)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:330)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:273)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:123)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:123)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:111)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:93)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3453)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4377)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3163)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4368)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:809)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4366)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4366)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3163)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:314)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:353)\n\tat sun.reflect.GeneratedMethodAccessor572.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n","errorSummary":"org.apache.spark.SparkException: [INTERNAL_ERROR] Cannot generate code for expression: input[1, string, true] ASC NULLS FIRST","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n","File \u001B[0;32m<command-303549684801381>:40\u001B[0m\n","\u001B[1;32m     36\u001B[0m df\u001B[38;5;241m.\u001B[39morderBy(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39masc(),col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdesc())\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n","\u001B[1;32m     39\u001B[0m df\u001B[38;5;241m.\u001B[39mcreateOrReplaceTempView(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEMP\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\u001B[0;32m---> 40\u001B[0m df\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memployee_name\u001B[39m\u001B[38;5;124m\"\u001B[39m,asc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepartment\u001B[39m\u001B[38;5;124m\"\u001B[39m),desc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate\u001B[39m\u001B[38;5;124m\"\u001B[39m),\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mage\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbonus\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n","\u001B[1;32m     42\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect employee_name,department,state,salary,age,bonus from EMP ORDER BY department asc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mshow(truncate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:933\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n","\u001B[1;32m    924\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n","\u001B[1;32m    925\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n","\u001B[1;32m    926\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_A_BOOLEAN\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n","\u001B[1;32m    927\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\n","\u001B[0;32m   (...)\u001B[0m\n","\u001B[1;32m    930\u001B[0m         },\n","\u001B[1;32m    931\u001B[0m     )\n","\u001B[0;32m--> 933\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mint_truncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n","\n","File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n","\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n","\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n","\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n","\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n","\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","\n","File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n","\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n","\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n","\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n","\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n","\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n","\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n","\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n","\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n","\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o485.showString.\n",": org.apache.spark.SparkException: [INTERNAL_ERROR] Cannot generate code for expression: input[1, string, true] ASC NULLS FIRST\n","\tat org.apache.spark.SparkException$.internalError(SparkException.scala:78)\n","\tat org.apache.spark.SparkException$.internalError(SparkException.scala:82)\n","\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGenerateCodeForExpressionError(QueryExecutionErrors.scala:74)\n","\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:489)\n","\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:488)\n","\tat org.apache.spark.sql.catalyst.expressions.SortOrder.doGenCode(SortOrder.scala:63)\n","\tat org.apache.spark.sql.catalyst.expressions.Expression.genCodeInternal(Expression.scala:235)\n","\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$2(Expression.scala:211)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:211)\n","\tat org.apache.spark.sql.catalyst.expressions.Alias.genCodeInternal(namedExpressions.scala:163)\n","\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$2(EdgeExpressionCodegen.scala:269)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.$anonfun$genCodeWithFallback$1(EdgeExpressionCodegen.scala:269)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat com.databricks.sql.expressions.codegen.EdgeExpressionCodegen$.genCodeWithFallback(EdgeExpressionCodegen.scala:267)\n","\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpression(CodeGenerator.scala:1450)\n","\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$2(CodeGenerator.scala:1531)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressionsForWholeStageWithCSE$1(CodeGenerator.scala:1529)\n","\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1183)\n","\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressionsForWholeStageWithCSE(CodeGenerator.scala:1529)\n","\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n","\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:197)\n","\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:152)\n","\tat org.apache.spark.sql.execution.RDDScanExec.consume(ExistingRDD.scala:297)\n","\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:488)\n","\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:461)\n","\tat org.apache.spark.sql.execution.RDDScanExec.doProduce(ExistingRDD.scala:297)\n","\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:98)\n","\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:108)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:334)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:330)\n","\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:93)\n","\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:93)\n","\tat org.apache.spark.sql.execution.RDDScanExec.produce(ExistingRDD.scala:297)\n","\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:56)\n","\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:98)\n","\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:108)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:334)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:330)\n","\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:93)\n","\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:93)\n","\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:46)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:661)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:724)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:277)\n","\tat org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:108)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:334)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:330)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:273)\n","\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:123)\n","\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n","\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:123)\n","\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:111)\n","\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:93)\n","\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n","\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n","\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n","\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n","\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n","\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n","\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3453)\n","\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4377)\n","\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3163)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4368)\n","\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:809)\n","\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4366)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:227)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:410)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:172)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1038)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:122)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:360)\n","\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4366)\n","\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3163)\n","\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3384)\n","\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:314)\n","\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:353)\n","\tat sun.reflect.GeneratedMethodAccessor572.invoke(Unknown Source)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n","\tat py4j.Gateway.invoke(Gateway.java:306)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n","\tat java.lang.Thread.run(Thread.java:750)\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ea3f7b22-a0f8-4245-b806-e5af9a5a15f6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-orderby.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
