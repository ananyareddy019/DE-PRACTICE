{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\ndata = [('James','Smith','M',3000),\n  ('Anna','Rose','F',4100),\n  ('Robert','Williams','M',6200), \n]\n\ncolumns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\ndf = spark.createDataFrame(data=data, schema = columns)\ndf.show()\n\n#Example 1 mapPartitions()\ndef reformat(partitionData):\n    for row in partitionData:\n        yield [row.firstname+\",\"+row.lastname,row.salary*10/100]\ndf.rdd.mapPartitions(reformat).toDF().show()\n\n#Example 2 mapPartitions()\ndef reformat2(partitionData):\n  updatedData = []\n  for row in partitionData:\n    name=row.firstname+\",\"+row.lastname\n    bonus=row.salary*10/100\n    updatedData.append([name,bonus])\n  return iter(updatedData)\n\ndf2=df.rdd.mapPartitions(reformat2).toDF(\"name\",\"bonus\")\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"150d8191-df1e-4911-a765-10f7759827ed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+--------+------+------+\n|firstname|lastname|gender|salary|\n+---------+--------+------+------+\n|    James|   Smith|     M|  3000|\n|     Anna|    Rose|     F|  4100|\n|   Robert|Williams|     M|  6200|\n+---------+--------+------+------+\n\n+---------------+-----+\n|             _1|   _2|\n+---------------+-----+\n|    James,Smith|300.0|\n|      Anna,Rose|410.0|\n|Robert,Williams|620.0|\n+---------------+-----+\n\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-303549684801372>:27\u001B[0m\n\u001B[1;32m     24\u001B[0m     updatedData\u001B[38;5;241m.\u001B[39mappend([name,bonus])\n\u001B[1;32m     25\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28miter\u001B[39m(updatedData)\n\u001B[0;32m---> 27\u001B[0m df2\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mmapPartitions(reformat2)\u001B[38;5;241m.\u001B[39mtoDF(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbonus\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:113\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    +---+\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1181\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n\u001B[1;32m   1178\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata is already a DataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m-> 1181\u001B[0m     schema \u001B[38;5;241m=\u001B[39m cast(Union[AtomicType, StructType, \u001B[38;5;28mstr\u001B[39m], \u001B[43m_parse_datatype_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1182\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[1;32m   1183\u001B[0m     \u001B[38;5;66;03m# Must re-encode any unicode strings to be consistent with StructField names\u001B[39;00m\n\u001B[1;32m   1184\u001B[0m     schema \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m schema]\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1144\u001B[0m, in \u001B[0;36m_parse_datatype_string\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m   1142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m from_ddl_datatype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstruct<\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m s\u001B[38;5;241m.\u001B[39mstrip())\n\u001B[1;32m   1143\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:\n\u001B[0;32m-> 1144\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1134\u001B[0m, in \u001B[0;36m_parse_datatype_string\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m   1128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _parse_datatype_json_string(\n\u001B[1;32m   1129\u001B[0m         sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mpython\u001B[38;5;241m.\u001B[39mPythonSQLUtils\u001B[38;5;241m.\u001B[39mparseDataType(type_str)\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m   1130\u001B[0m     )\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1133\u001B[0m     \u001B[38;5;66;03m# DDL format, \"fieldname datatype, fieldname datatype\".\u001B[39;00m\n\u001B[0;32m-> 1134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrom_ddl_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1135\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1137\u001B[0m         \u001B[38;5;66;03m# For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1123\u001B[0m, in \u001B[0;36m_parse_datatype_string.<locals>.from_ddl_schema\u001B[0;34m(type_str)\u001B[0m\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_ddl_schema\u001B[39m(type_str: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataType:\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m sc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _parse_datatype_json_string(\n\u001B[0;32m-> 1123\u001B[0m         \u001B[43msc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mStructType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromDDL\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtype_str\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m   1124\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 1, pos 4)\n\n== SQL ==\nname\n----^^^\n","errorSummary":"<span class='ansi-red-fg'>ParseException</span>: \n[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 1, pos 4)\n\n== SQL ==\nname\n----^^^\n","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\n","File \u001B[0;32m<command-303549684801372>:27\u001B[0m\n","\u001B[1;32m     24\u001B[0m     updatedData\u001B[38;5;241m.\u001B[39mappend([name,bonus])\n","\u001B[1;32m     25\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28miter\u001B[39m(updatedData)\n","\u001B[0;32m---> 27\u001B[0m df2\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mrdd\u001B[38;5;241m.\u001B[39mmapPartitions(reformat2)\u001B[38;5;241m.\u001B[39mtoDF(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbonus\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\u001B[1;32m     28\u001B[0m df2\u001B[38;5;241m.\u001B[39mshow()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:113\u001B[0m, in \u001B[0;36m_monkey_patch_RDD.<locals>.toDF\u001B[0;34m(self, schema, sampleRatio)\u001B[0m\n","\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@no_type_check\u001B[39m\n","\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtoDF\u001B[39m(\u001B[38;5;28mself\u001B[39m, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sampleRatio\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n","\u001B[1;32m     80\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n","\u001B[1;32m     81\u001B[0m \u001B[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001B[39;00m\n","\u001B[1;32m     82\u001B[0m \n","\u001B[0;32m   (...)\u001B[0m\n","\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    +---+\u001B[39;00m\n","\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n","\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreateDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampleRatio\u001B[49m\u001B[43m)\u001B[49m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n","\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n","\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n","\u001B[1;32m     51\u001B[0m     )\n","\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1181\u001B[0m, in \u001B[0;36mSparkSession.createDataFrame\u001B[0;34m(self, data, schema, samplingRatio, verifySchema)\u001B[0m\n","\u001B[1;32m   1178\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata is already a DataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n","\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, \u001B[38;5;28mstr\u001B[39m):\n","\u001B[0;32m-> 1181\u001B[0m     schema \u001B[38;5;241m=\u001B[39m cast(Union[AtomicType, StructType, \u001B[38;5;28mstr\u001B[39m], \u001B[43m_parse_datatype_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m)\n","\u001B[1;32m   1182\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(schema, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n","\u001B[1;32m   1183\u001B[0m     \u001B[38;5;66;03m# Must re-encode any unicode strings to be consistent with StructField names\u001B[39;00m\n","\u001B[1;32m   1184\u001B[0m     schema \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m schema]\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1144\u001B[0m, in \u001B[0;36m_parse_datatype_string\u001B[0;34m(s)\u001B[0m\n","\u001B[1;32m   1142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m from_ddl_datatype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstruct<\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m s\u001B[38;5;241m.\u001B[39mstrip())\n","\u001B[1;32m   1143\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m:\n","\u001B[0;32m-> 1144\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1134\u001B[0m, in \u001B[0;36m_parse_datatype_string\u001B[0;34m(s)\u001B[0m\n","\u001B[1;32m   1128\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _parse_datatype_json_string(\n","\u001B[1;32m   1129\u001B[0m         sc\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39msql\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mpython\u001B[38;5;241m.\u001B[39mPythonSQLUtils\u001B[38;5;241m.\u001B[39mparseDataType(type_str)\u001B[38;5;241m.\u001B[39mjson()\n","\u001B[1;32m   1130\u001B[0m     )\n","\u001B[1;32m   1132\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[1;32m   1133\u001B[0m     \u001B[38;5;66;03m# DDL format, \"fieldname datatype, fieldname datatype\".\u001B[39;00m\n","\u001B[0;32m-> 1134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfrom_ddl_schema\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1135\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n","\u001B[1;32m   1136\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n","\u001B[1;32m   1137\u001B[0m         \u001B[38;5;66;03m# For backwards compatibility, \"integer\", \"struct<fieldname: datatype>\" and etc.\u001B[39;00m\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:1123\u001B[0m, in \u001B[0;36m_parse_datatype_string.<locals>.from_ddl_schema\u001B[0;34m(type_str)\u001B[0m\n","\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_ddl_schema\u001B[39m(type_str: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataType:\n","\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m sc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m sc\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n","\u001B[1;32m   1122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _parse_datatype_json_string(\n","\u001B[0;32m-> 1123\u001B[0m         \u001B[43msc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mStructType\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromDDL\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtype_str\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mjson()\n","\u001B[1;32m   1124\u001B[0m     )\n","\n","File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n","\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n","\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n","\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n","\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n","\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n","\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","\n","File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n","\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n","\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n","\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n","\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n","\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n","\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n","\n","\u001B[0;31mParseException\u001B[0m: \n","[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 1, pos 4)\n","\n","== SQL ==\n","name\n","----^^^\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7aed2422-f41a-4b04-a6a8-23a3ec3b1c09","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-mappartitions.py","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
